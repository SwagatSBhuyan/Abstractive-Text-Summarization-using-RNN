# Abstract
Automated text summarization systems require being heedful of the reader and the communication goals since it may determine whether the original textual content is worth reading in full. The summary can also assist in enhancing document indexing for information retrieval, and it is generally much less biased than a human-written summary. A crucial part of building intelligent systems is evaluating them. Consequently, the choice of evaluation metric(s) is of utmost importance. Although fairly effective for evaluating extractive text summarization systems, standard evaluation metrics like BLEU and ROUGE become futile when comparing semantic information between two texts, i.e., in abstractive summarization. We propose textual entailment as a potential metric to evaluate abstractive summaries. The results show the contribution of text entailment as a strong automated evaluation model for such summaries. The textual entailment scores between the text and generated summaries, as well as between the reference and predicted summaries, were calculated, and an overall summarizer score was generated to give a fair idea of how efficient the generated summaries are. We put forward some novel methods that use the entailment scores and the final summarizer scores for a reasonable evaluation of the same across various scenarios. A Final Entailment Metric Score (FEMS) was generated to get an insightful idea in order to compare both the generated summaries.
# Publication link
Natural Language Processing Journal - Elsevier: https://www.sciencedirect.com/science/article/pii/S2949719123000250
**********************************************************************************************************************
