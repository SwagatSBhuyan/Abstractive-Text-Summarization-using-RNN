{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JN1CXyeuleNS",
    "outputId": "4b4e48fa-c708-482c-eef6-ad4be8bd4265"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WpMYh00ik9qm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swaga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swaga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swaga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\swaga\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from preprocess import *\n",
    "from vocabulary import *\n",
    "from model import *\n",
    "from model_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swaga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swaga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swaga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVaFT9sBnKVi"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('../../../evaluation/snli/snli_/snli_1.0_train.csv')\n",
    "# val_df   = pd.read_csv('../../../evaluation/snli/snli_/snli_1.0_dev.csv')\n",
    "# # train_df = pd.read_csv('../datasets/snli/snli_/snli_1.0_train.csv')\n",
    "# # val_df   = pd.read_csv('../datasets/snli/snli_/snli_1.0_dev.csv')\n",
    "# train_df, val_df = preprocess_text(train_df, val_df)\n",
    "# print(len(train_df))\n",
    "# print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_df['sentence2'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "l2TMjBJDF1uT",
    "outputId": "8b9cda3b-9519-4504-e8ee-c527011f783d"
   },
   "outputs": [],
   "source": [
    "# rw = pd.read_csv('../datasets/Transformer_generated_summaries.csv', encoding='utf-8')\n",
    "# rw = pd.read_csv('ins.csv', encoding='utf-8')\n",
    "rw = pd.read_csv('test_sets/ins.csv', encoding='ansi')\n",
    "# rw = pd.read_csv('../datasets/test_data/Transformer_generated_summaries.csv', encoding='utf-8')\n",
    "\n",
    "rw.drop(['original_summary'], axis = 1, inplace=True)\n",
    "rw.rename(columns = {'text':'sentence1'}, inplace = True)\n",
    "rw.rename(columns = {'Predicted_summary':'sentence2'}, inplace = True)\n",
    "\n",
    "# print(rw)\n",
    "gold = [-1] * 100\n",
    "rw.insert(loc=0, column='gold_label', value=gold)\n",
    "rw = rw[['gold_label', 'sentence1', 'sentence2']]\n",
    "# rw = rw[['sentence1', 'sentence2']]\n",
    "rw = rw.dropna()\n",
    "rw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rw = pd.read_csv('../datasets/Transformer_generated_summaries.csv', encoding='utf-8')\n",
    "# rw = pd.read_csv('ins.csv', encoding='utf-8')\n",
    "rw_ = pd.read_csv('test_sets/ins.csv', encoding='ansi')\n",
    "# rw = pd.read_csv('../datasets/test_data/Transformer_generated_summaries.csv', encoding='utf-8')\n",
    "\n",
    "rw_.drop(['text'], axis = 1, inplace=True)\n",
    "rw_.rename(columns = {'original_summary':'sentence1'}, inplace = True)\n",
    "rw_.rename(columns = {'Predicted_summary':'sentence2'}, inplace = True)\n",
    "\n",
    "gold = [-1] * 100\n",
    "rw_.insert(loc=0, column='gold_label', value=gold)\n",
    "rw_ = rw_[['gold_label', 'sentence1', 'sentence2']]\n",
    "# rw = rw[['sentence1', 'sentence2']]\n",
    "rw_ = rw_.dropna()\n",
    "rw_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw['sentence1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_['sentence2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rw['sentence2'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlhVFSEXntjC"
   },
   "outputs": [],
   "source": [
    "# train_df['sentence1'] = train_df['sentence1'].astype(str).apply(lambda text: clean_text(text))\n",
    "# train_df['sentence2'] = train_df['sentence2'].astype(str).apply(lambda text: clean_text(text))\n",
    "# val_df['sentence1'] = val_df['sentence1'].astype(str).apply(lambda text: clean_text(text))\n",
    "# val_df['sentence2'] = val_df['sentence2'].astype(str).apply(lambda text: clean_text(text))\n",
    "rw['sentence1'] = rw['sentence1'].astype(str).apply(lambda text: clean_text(text))\n",
    "rw['sentence2'] = rw['sentence2'].astype(str).apply(lambda text: clean_text(text))\n",
    "rw_['sentence1'] = rw_['sentence1'].astype(str).apply(lambda text: clean_text(text))\n",
    "rw_['sentence2'] = rw_['sentence2'].astype(str).apply(lambda text: clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[(train_df['sentence1'].str.split().str.len() > 0) & (train_df['sentence2'].str.split().str.len() > 0)]\n",
    "# val_df = val_df[(val_df['sentence1'].str.split().str.len() > 0) & (val_df['sentence2'].str.split().str.len() > 0)]\n",
    "# print(train_df[(train_df['sentence1'].str.split().str.len() == 0) | (train_df['sentence2'].str.split().str.len() == 0)])\n",
    "# print(val_df[(val_df['sentence1'].str.split().str.len() == 0) | (val_df['sentence2'].str.split().str.len() == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "HDGOe_fln-hK",
    "outputId": "b7b3299e-c3ab-4c24-a277-07e42e904efb"
   },
   "outputs": [],
   "source": [
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "C1W3s5Ojn_mX",
    "outputId": "a7b88a51-54b3-4fed-d2ff-52ba129a56d8"
   },
   "outputs": [],
   "source": [
    "# val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1g-SvVOoAfP"
   },
   "outputs": [],
   "source": [
    "# train_val_df = pd.concat([train_df, val_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eihUwlHHoDZm"
   },
   "outputs": [],
   "source": [
    "# sentence_pairs, _ = pair_generator(train_val_df)\n",
    "rw_sentence_pairs, __ = pair_generator(rw)\n",
    "rw_sentence_pairs_, __ = pair_generator(rw_)\n",
    "# train_sentence_pairs, train_sentence_labels = pair_generator(train_df)\n",
    "# val_sentence_pairs, val_sentence_labels = pair_generator(val_df)\n",
    "\n",
    "# labels = set(train_sentence_labels)\n",
    "# print(labels)\n",
    "\n",
    "# tag2idx = {word: i for i, word in enumerate(labels)}\n",
    "# print(tag2idx)\n",
    "tag2idx = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "print(tag2idx)\n",
    "\n",
    "# train_labels = [tag2idx[t] for t in train_sentence_labels]\n",
    "# val_labels = [tag2idx[t] for t in val_sentence_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_sentence_pairs_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPSLm_XnoPKk",
    "outputId": "a79ec791-7bb5-4060-e26f-f0b08d5f7be8"
   },
   "outputs": [],
   "source": [
    "# vocab = Vocabulary()\n",
    "\n",
    "# for data in [rw_sentence_pairs]:\n",
    "#   for sen in data:\n",
    "#     premise    = sen[0]\n",
    "#     hypothesis = sen[1]\n",
    "#     vocab.addSentence(premise)\n",
    "#     vocab.addSentence(hypothesis)\n",
    "    \n",
    "# for data in [rw_sentence_pairs_]:\n",
    "#   for sen in data:\n",
    "#     premise    = sen[0]\n",
    "#     hypothesis = sen[1]\n",
    "#     vocab.addSentence(premise)\n",
    "#     vocab.addSentence(hypothesis)\n",
    "\n",
    "# for data in [sentence_pairs]:\n",
    "#   for sentence_pair in data:\n",
    "#     premise    = sentence_pair[0]\n",
    "#     hypothesis = sentence_pair[1]\n",
    "#     vocab.addSentence(premise)\n",
    "#     vocab.addSentence(hypothesis)\n",
    "\n",
    "# with open('saved_vocabs/voc.class', 'wb') as vocab_file:\n",
    "#     pickle.dump(vocab, vocab_file)\n",
    "# print(\"Vocab size:\", len(vocab.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_vocabs/voc.class', 'rb') as vocab_file_r:\n",
    "    vocab = pickle.load(vocab_file_r)\n",
    "print(\"Vocab size:\", len(vocab.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neSP5WajoSat"
   },
   "outputs": [],
   "source": [
    "index2word = {}\n",
    "for wrd, idx in vocab.word2index.items():\n",
    "    # print(wrd, idx)\n",
    "    index2word[idx] = wrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8t7_OJcozF0"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 300\n",
    "VOCAB_SIZE = len(vocab.word2index)\n",
    "TARGET_SIZE = len(tag2idx)\n",
    "HIDDEN_SIZE = 128\n",
    "LEARNING_RATE = 0.005\n",
    "STACKED_LAYERS = 2\n",
    "EMBEDDING_PATH = '../../../../embeddings/google_news/GoogleNews-vectors-negative300.bin'\n",
    "GLOVE_EMBEDDING = '../../../../embeddings/glove/glove.6B.300d.txt'\n",
    "# EMBEDDING_PATH = '../embeddings/google_news/GoogleNews-vectors-negative300.bin'\n",
    "# GLOVE_EMBEDDING = '../embeddings/glove/glove.6B.300d.txt'\n",
    "\n",
    "initiate_model_vocab(vocab, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZKtFMQ6oTVk"
   },
   "outputs": [],
   "source": [
    "train_data = DataSetLoader(get_pair_indices(vocab, train_sentence_pairs), train_labels)\n",
    "val_data   = DataSetLoader(get_pair_indices(vocab, val_sentence_pairs), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UoB3xqDyRLG",
    "outputId": "505953f5-3b5c-48e5-a52c-ea0980e577a2"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = BATCH_SIZE, collate_fn=lambda x:x)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, batch_size = BATCH_SIZE, collate_fn=lambda x:x)\n",
    "\n",
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m4Riv3kzvGI"
   },
   "outputs": [],
   "source": [
    "embeddings_index = load_embeddings(GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvnD6Q6yzwbJ",
    "outputId": "17464d30-1cb3-422d-8598-bdc1d177ae81"
   },
   "outputs": [],
   "source": [
    "weights = 1 * np.random.randn(VOCAB_SIZE + 1, EMBEDDING_SIZE)\n",
    "embedded_count = 0\n",
    "for word, lang_word_index in vocab.word2index.items():\n",
    "  if embeddings_index.get(word) is not None:\n",
    "    weights[lang_word_index] = embeddings_index.get(word)\n",
    "    embedded_count += 1\n",
    "\n",
    "print(\"Embedded count:\", embedded_count)\n",
    "del embeddings_index\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9fEl77ezx5X",
    "outputId": "5f001020-d2de-4d7a-c74c-ba3a00b50010"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM(VOCAB_SIZE, HIDDEN_SIZE, TARGET_SIZE, STACKED_LAYERS, weights, True)\n",
    "lstm_model.to(device)\n",
    "print(lstm_model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(lstm_model, train_loader, val_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in lstm_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", lstm_model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model, 'sm_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsH495dMz4Xu",
    "outputId": "0632ab0c-b244-4bab-bd1d-e24426d6b445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(33589, 300)\n",
       "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (FC_concat1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (FC_concat2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (FC_concat3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (output): Linear(in_features=32, out_features=3, bias=True)\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = torch.load('saved_models/sm_2')\n",
    "lstm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkfY79IF0Eav",
    "outputId": "d0797bd8-350e-4e8d-a8cd-bb39afe9d168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=32, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(lstm_model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(f'runs/SNLI/tensorboard_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vufrSKr90mt0",
    "outputId": "ba80614c-2cb8-4d80-c941-c770f779b7c4"
   },
   "outputs": [],
   "source": [
    "sentence_o = [''] * len(rw)\n",
    "sentence_p = [''] * len(rw)\n",
    "# sentence_o = ['']\n",
    "# sentence_p = ['']\n",
    "\n",
    "# sentence_o[0] = \"rock music is very good for public\"\n",
    "# sentence_p[0] = \"people dont like rock and roll\"\n",
    "# sentence_o = \"uk india business council chairperson said 60 british firms likely increase investment india next years firms positive reforms fdi introduced last two years added india even important britain said\"\n",
    "# sentence_p = \"60 british firms to increase investment in india\"\n",
    "# sentence_o = clean_text(sentence_o)\n",
    "# sentence_p = clean_text(sentence_p)\n",
    "\n",
    "\n",
    "# sen_p1 = [(sentence_o, sentence_p)]\n",
    "# # sen_p2 = []\n",
    "# for i in range(rw):\n",
    "#   sen1 = sentence_o[i]\n",
    "#   sen2 = sentence_p[i]\n",
    "#   sen_p1.append((sen1, sen2))\n",
    "# for i in range(rw_):\n",
    "#   sen2 = sentence_o[i]\n",
    "#   sen1 = sentence_p[i]\n",
    "#   sen_p2.append((sen1, sen2))\n",
    "\n",
    "# sen_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5QABAtRQynp",
    "outputId": "d2b0fd48-e128-482f-fff4-005d59b99896"
   },
   "outputs": [],
   "source": [
    "rrw_sentence_pairs_ = []\n",
    "for i in range(len(rw_sentence_pairs_)):\n",
    "    rrw_sentence_pairs_.append((rw_sentence_pairs_[i][1], rw_sentence_pairs_[i][0]))\n",
    "print('rw[0]: ', rw_sentence_pairs_[0])\n",
    "print('rrw[0]: ', rrw_sentence_pairs_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rw_sentence_pairs, __ = pair_generator(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKnftM-cSWiC",
    "outputId": "9d57aebc-60f6-4348-cef1-bd37278456ac"
   },
   "outputs": [],
   "source": [
    "id_pairs = get_pair_indices(vocab, rw_sentence_pairs)\n",
    "id_pairs_1 = get_pair_indices(vocab, rw_sentence_pairs_)\n",
    "id_pairs_2 = get_pair_indices(vocab, rrw_sentence_pairs_)\n",
    "# id_pairs = get_pair_indices(vocab, sen_p1)\n",
    "# id_pairs = get_pair_indices(vocab, sen_p2)\n",
    "# id_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzurbdbC0pe9",
    "outputId": "e65643f5-9519-47e0-87b7-bd03ce07a9ea"
   },
   "outputs": [],
   "source": [
    "premise_seq    = [torch.tensor(seq[0]).long().to(device) for seq in id_pairs]\n",
    "hypothesis_seq = [torch.tensor(seq[1]).long().to(device) for seq in id_pairs]\n",
    "\n",
    "premise_len    = list(map(len, premise_seq))\n",
    "hypothesis_len = list(map(len, hypothesis_seq))\n",
    "\n",
    "batch = len(premise_seq)\n",
    "temp = pad_sequence(premise_seq + hypothesis_seq, batch_first=True)\n",
    "premise_seq    = temp[:batch, :]\n",
    "hypothesis_seq = temp[batch:, :]\n",
    "\n",
    "prediction = lstm_model([premise_seq, hypothesis_seq], premise_len, hypothesis_len)\n",
    "# prediction = prediction[prediction!=prediction[0,3]]\n",
    "\n",
    "prediction_list = prediction.to('cpu')\n",
    "prediction_list = prediction_list.tolist()\n",
    "print(len(prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_seq    = [torch.tensor(seq[0]).long().to(device) for seq in id_pairs_1]\n",
    "hypothesis_seq = [torch.tensor(seq[1]).long().to(device) for seq in id_pairs_1]\n",
    "\n",
    "premise_len    = list(map(len, premise_seq))\n",
    "hypothesis_len = list(map(len, hypothesis_seq))\n",
    "\n",
    "batch = len(premise_seq)\n",
    "temp = pad_sequence(premise_seq + hypothesis_seq, batch_first=True)\n",
    "premise_seq    = temp[:batch, :]\n",
    "hypothesis_seq = temp[batch:, :]\n",
    "\n",
    "prediction_1 = lstm_model([premise_seq, hypothesis_seq], premise_len, hypothesis_len)\n",
    "# prediction = prediction[prediction!=prediction[0,3]]\n",
    "\n",
    "prediction_list = prediction_1.to('cpu')\n",
    "prediction_list = prediction_list.tolist()\n",
    "print(len(prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_seq    = [torch.tensor(seq[0]).long().to(device) for seq in id_pairs_2]\n",
    "hypothesis_seq = [torch.tensor(seq[1]).long().to(device) for seq in id_pairs_2]\n",
    "\n",
    "premise_len    = list(map(len, premise_seq))\n",
    "hypothesis_len = list(map(len, hypothesis_seq))\n",
    "\n",
    "batch = len(premise_seq)\n",
    "temp = pad_sequence(premise_seq + hypothesis_seq, batch_first=True)\n",
    "premise_seq    = temp[:batch, :]\n",
    "hypothesis_seq = temp[batch:, :]\n",
    "\n",
    "prediction_2 = lstm_model([premise_seq, hypothesis_seq], premise_len, hypothesis_len)\n",
    "# prediction = prediction[prediction!=prediction[0,3]]\n",
    "\n",
    "prediction_list = prediction_2.to('cpu')\n",
    "prediction_list = prediction_list.tolist()\n",
    "print(len(prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft = torch.log_softmax(prediction, dim=1).argmax(dim=1)\n",
    "soft.tolist()\n",
    "soft_1 = torch.log_softmax(prediction_1, dim=1).argmax(dim=1)\n",
    "soft_1.tolist()\n",
    "soft_2 = torch.log_softmax(prediction_2, dim=1).argmax(dim=1)\n",
    "soft_2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soft, soft_1, soft_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "for i in range(len(soft)):\n",
    "    if soft[i] == 2:\n",
    "        print(rw_sentence_pairs[i])\n",
    "        ctr = ctr + 1\n",
    "print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_1 = 0\n",
    "for i in range(len(soft_1)):\n",
    "    if soft_1[i] == 2:\n",
    "        print(rw_sentence_pairs_[i])\n",
    "        ctr_1 = ctr_1 + 1\n",
    "print(ctr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_2 = 0\n",
    "for i in range(len(soft_2)):\n",
    "    if soft_2[i] == 0:\n",
    "        print(rrw_sentence_pairs_[i])\n",
    "        ctr_2 = ctr_2 + 1\n",
    "print(ctr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_pairs = get_pair_indices(vocab, rrw_sentence_pairs)\n",
    "# # id_pairs = get_pair_indices(vocab, sen_p2)\n",
    "# id_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# premise_seq    = [torch.tensor(seq[0]).long().to(device) for seq in id_pairs]\n",
    "# hypothesis_seq = [torch.tensor(seq[1]).long().to(device) for seq in id_pairs]\n",
    "\n",
    "# premise_len    = list(map(len, premise_seq))\n",
    "# hypothesis_len = list(map(len, hypothesis_seq))\n",
    "\n",
    "# batch = len(premise_seq)\n",
    "# temp = pad_sequence(premise_seq + hypothesis_seq, batch_first=True)\n",
    "# premise_seq    = temp[:batch, :]\n",
    "# hypothesis_seq = temp[batch:, :]\n",
    "\n",
    "# prediction = lstm_model([premise_seq, hypothesis_seq], premise_len, hypothesis_len)\n",
    "# # prediction = prediction[prediction!=prediction[0,3]]\n",
    "\n",
    "# prediction_list = prediction.to('cpu')\n",
    "# prediction_list = prediction_list.tolist()\n",
    "# print(len(prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft2 = torch.log_softmax(prediction, dim=1).argmax(dim=1)\n",
    "# soft2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctr2 = 0\n",
    "# for i in range(len(soft)):\n",
    "#     if soft2[i] == 0:\n",
    "#         print(rrw_sentence_pairs[i])\n",
    "#         ctr2 = ctr2 + 1\n",
    "# print(ctr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_100 = 0   # perfectly entailed pairs\n",
    "e_50 = 0    # entailment\n",
    "n_100 = 0   # perfectly neutral pairs\n",
    "n_50 = 0    # neutral\n",
    "cc = 0      # contradicting pairs\n",
    "for i in range(len(soft_1)):\n",
    "    if soft_1[i] == soft_2[i] == 0:\n",
    "        e_100 = e_100 + 1\n",
    "    elif (soft_1[i] == 0 and soft_2[i] == 1) or (soft_1[i] == 1 and soft_2[i] == 0) or (soft_1[i] == 0 and soft_2[i] == 2) or (soft_1[i] == 2 and soft_2[i] == 0):\n",
    "        e_50 = e_50 + 1\n",
    "    elif soft_1[i] == soft_2[i] == 1:\n",
    "        n_100 = n_100 + 1\n",
    "    elif (soft_1[i] == 1 and soft_2[i] == 2) or (soft_1[i] == 2 and soft_2[i] == 1):\n",
    "        n_50 = n_50 + 1\n",
    "    else: \n",
    "        cc = cc + 1\n",
    "print('e100: ', e_100)\n",
    "print('e50: ', e_50)\n",
    "print('n100: ', n_100)\n",
    "print('n50: ', n_50)\n",
    "print('c100: ', cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_sentence_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_sentence_pairs[1]\n",
    "# 0: entailment\n",
    "# 1: neutral\n",
    "# 2: contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = torch.max(prediction, dim=0)\n",
    "mini = torch.min(prediction, dim=0)\n",
    "# print(maxi[0][0])\n",
    "denom1 = float(maxi[0][0].to('cpu')) - float(mini[0][0].to('cpu'))\n",
    "denom2 = float(maxi[0][1].to('cpu')) - float(mini[0][1].to('cpu'))\n",
    "denom3 = float(maxi[0][2].to('cpu')) - float(mini[0][2].to('cpu'))\n",
    "denom = ( denom1 + denom2 + denom3 ) / 3\n",
    "print(maxi, mini)\n",
    "print(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []\n",
    "for i in prediction:\n",
    "    a = []\n",
    "    for j in i:\n",
    "        a.append(float(j.to('cpu'))/denom)\n",
    "    aa.append(a)\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_scores = []\n",
    "for i in aa:\n",
    "    e_scores.append((i[0] + i[1]*0.5 - i[2])/2.5)\n",
    "print(e_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_labels = []\n",
    "for i in e_scores:\n",
    "    if i < 0:\n",
    "        E_labels.append('NEGATIVE ENTAILMENT')\n",
    "    elif i > 0.3:\n",
    "        E_labels.append('PERFECT ENTAILMENT')\n",
    "    else:\n",
    "        E_labels.append('NEUTRAL ENTAILMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrr = 0\n",
    "for i in e_scores:\n",
    "    if i < 0:\n",
    "        ctrr = ctrr + 1\n",
    "print(ctrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_E = pd.DataFrame(list(zip(rw_sentence_pairs, aa, e_scores, E_labels)), columns =['sentence pairs', 'entailment scores (raw: e, n, c)', 'entailment_metric [-1:1]', 'entailment_labels'])\n",
    "df_final_E.to_csv('scores/summary_entailment_scores_of_test_pairs.csv')\n",
    "df_final_E.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi_1 = torch.max(prediction_1, dim=0)\n",
    "mini_1 = torch.min(prediction_1, dim=0)\n",
    "# print(maxi[0][0])\n",
    "denom1_1 = float(maxi_1[0][0].to('cpu')) - float(mini_1[0][0].to('cpu'))\n",
    "denom2_1 = float(maxi_1[0][1].to('cpu')) - float(mini_1[0][1].to('cpu'))\n",
    "denom3_1 = float(maxi_1[0][2].to('cpu')) - float(mini_1[0][2].to('cpu'))\n",
    "denom_1 = ( denom1_1 + denom2_1 + denom3_1 ) / 3\n",
    "print(maxi_1, mini_1)\n",
    "print(denom_1)\n",
    "\n",
    "maxi_2 = torch.max(prediction_2, dim=0)\n",
    "mini_2 = torch.min(prediction_2, dim=0)\n",
    "# print(maxi[0][0])\n",
    "denom1_2 = float(maxi_2[0][0].to('cpu')) - float(mini_2[0][0].to('cpu'))\n",
    "denom2_2 = float(maxi_2[0][1].to('cpu')) - float(mini_2[0][1].to('cpu'))\n",
    "denom3_2 = float(maxi_2[0][2].to('cpu')) - float(mini_2[0][2].to('cpu'))\n",
    "denom_2 = ( denom1 + denom2 + denom3 ) / 3\n",
    "print(maxi_2, mini_2)\n",
    "print(denom_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = []\n",
    "for i in prediction_1:\n",
    "    p = []\n",
    "    for j in i:\n",
    "        p.append(float(j.to('cpu'))/denom_1)\n",
    "    p1.append(p)\n",
    "print(p1)\n",
    "\n",
    "p2 = []\n",
    "for i in prediction_2:\n",
    "    p = []\n",
    "    for j in i:\n",
    "        p.append(float(j.to('cpu'))/denom_2)\n",
    "    p2.append(p)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me1_scores = []\n",
    "for i in p1:\n",
    "    me1_scores.append((i[0] + i[1]*0.5 - i[2])/2.5)\n",
    "print(me1_scores)\n",
    "\n",
    "me2_scores = []\n",
    "for i in p2:\n",
    "    me2_scores.append((i[0] + i[1]*0.5 - i[2])/2.5)\n",
    "print(me2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_scores= []\n",
    "for i, j in zip(me1_scores, me2_scores):\n",
    "    mutual_scores.append( (i + j) / 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUTUAL_labels = []\n",
    "n_c = 0\n",
    "p_c = 0\n",
    "c_c = 0\n",
    "for i in mutual_scores:\n",
    "    if i <= -0.2:\n",
    "        c_c += 1\n",
    "        MUTUAL_labels.append('CONTRADICTION')\n",
    "    elif i > -0.2 and i < 0:\n",
    "        n_c += 1\n",
    "        MUTUAL_labels.append('NEGATIVE ENTAILMENT')\n",
    "    else:\n",
    "        p_c += 1\n",
    "        MUTUAL_labels.append('POSITIVE ENTAILMENT')\n",
    "        \n",
    "print(\"POSITIVE Entailments: \", p_c)\n",
    "print(\"NEGATIVE Entailments: \", n_c)\n",
    "print(\"Contradictions: \", c_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ME = pd.DataFrame(list(zip(rw_sentence_pairs, p1, p2, me1_scores, me2_scores, mutual_scores, MUTUAL_labels)), columns =['sentence pairs', 'e_n_c scores 1', 'e_n_c scores 2', 'entailment_metric_1[-1:1]', 'entailment_metric_2[-1:1]', 'mutuality_score', 'mutual_labels'])\n",
    "df_final_ME.to_csv('scores/mutual_entailment_scores_of_test_pairs.csv')\n",
    "df_final_ME.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "entail_lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
